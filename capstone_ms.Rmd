---
title: "Text prediction - milestone assignment of the capstone project"
author: "Evgeniy Zabrodskiy"
date: "17 March 2016"
output: 
  html_document: 
    keep_md: yes
---

# Synopsis
The goal of this analysis is to understand the distribution and relationship between the words, tokens, and phrases in the text in order to build a predictive model.
The distributions of frequencies of words, word pairs and word triplets are analised and shown in form of word clouds and barplots.

```{r, echo = FALSE, results = "hide", warning = FALSE, message = FALSE}
library(tm)
library(RWeka)
library(wordcloud)
library(ggplot2)
library(grid)
library(gridExtra)
library(stringi)

# wordcloud/RWeka does not work with parallel processing when NGramTokenizer is used.
options(mc.cores=1)
```

## Loading data
```{r Loading, cache = TRUE, echo = FALSE}
# this is needed for tokenizer in DocumentTermMatrix to work. For some reason does not with parallel processing.
# Found solution here: http://stackoverflow.com/questions/17703553/bigrams-instead-of-single-words-in-termdocument-matrix-using-r-and-rweka

ds <- DirSource(directory = "final/en_US/")
docs <- Corpus(ds, readerControl = list(reader = readPlain, language = "en", load = TRUE))

docs.num <- length(docs)

# gather simple information about documents corpus
docs.info.df <- data.frame(FileName = character(), 
                           NumberOfRows = integer(),
                           NumberOfWords = integer())
for(i in 1:docs.num) {
    docs.info.df <- rbind(docs.info.df, 
                          data.frame(FileName = docs[[i]]$meta$id, 
                                     NumberOfRows = length(docs[[i]]$content),
                                     NumberOfWords = sum(stri_count_words(docs[[i]]$content))))
}
```

Here are the loaded file names, their lenghts in number of rows and total number of words:
```{r, echo = FALSE}
docs.info.df
```

## Sampling and cleaning data
The files are quite big (more than 4 millions of rows in total) and building document term matrix using original files is computationally expensive and may take a lot of time. Besides, for the word prediction task we're interested in frequent unigrams, bigrams and trigrams which can be obtained from a smaller sample.  

```{r getSample, echo = FALSE}
sample.size <- as.integer(10000)
# copy original doc for processing (don't know how to make it more efficient yet)
docs.sample <- docs
# select random sample of sample.size from the original
# iterate through documents in the corpus
for (i in 1:docs.num) {
    # get length of each document (number of lines)
    l <- length(docs[[i]]$content)
    # get random sample of lines in the document and replace the original doc in the corpus with the smaller sample.
    docs.sample[[i]]$content <- docs[[i]]$content[sample(1:l, as.integer(sample.size))]
}
```

We randomly selected **`r sample.size`** rows from each document and all further cleaning and transformations are done using the sample.  

```{r cleanDocs, echo = FALSE}
# remove punctuation
docs.sample <- tm_map(docs.sample, removePunctuation)   

# remove numbers
docs.sample <- tm_map(docs.sample, removeNumbers)

# remove some special characters
#for(j in seq(docs.sample)) {
#    docs.sample[[j]]$content <- gsub("/", " ", docs.sample[[j]]$content)   
#    docs.sample[[j]]$content <- gsub("@", " ", docs.sample[[j]]$content)   
#    docs.sample[[j]]$content <- gsub("\\|", " ", docs.sample[[j]]$content)
#}   

# remove unnecessary whitespaces
docs.sample <- tm_map(docs.sample, stripWhitespace)

# to lowecase
docs.sample <- tm_map(docs.sample, content_transformer(tolower))

# finalize preprocessing
docs.sample <- tm_map(docs.sample, content_transformer(PlainTextDocument))
```

Cleaning procedures and transformations that were applied to the sample are:  
- remove punctuation,  
- remove numbers,  
- convert to lowercase,  
- remove unnecessary whitespaces.  

```{r DocumentTermMatrix, echo = FALSE}
##  create document term matrix for all documents
# Singles
dtm.sample.unigram <- DocumentTermMatrix(docs.sample)

# remove sparse terms
dtms.sample.unigram <- removeSparseTerms(dtm.sample.unigram, 0.1)

# Doubles
bigramTokenizer <- function(x) NGramTokenizer(x, 
                                Weka_control(min = 2, max = 2))

dtm.sample.bigram <- DocumentTermMatrix(docs.sample, control = list(tokenize = bigramTokenizer))

# remove sparse terms
dtms.sample.bigram <- removeSparseTerms(dtm.sample.bigram, 0.1)

# Triples
trigramTokenizer <- function(x) NGramTokenizer(x, 
                                Weka_control(min = 3, max = 3))

dtm.sample.trigram <- DocumentTermMatrix(docs.sample, control = list(tokenize = trigramTokenizer))

# remove sparse terms
dtms.sample.trigram <- removeSparseTerms(dtm.sample.trigram, 0.1)
```

## Exploratory analysis

### Basic information about the data
Once the data is clean, the document term matrix can be created. It contains the frequncies of terms in each document. The frequencies of terms can be converted to probabilities for prediction modelling at later stages of the project.  

Here is the default output of the document term matrix created from the data sample:  

```{r, echo = FALSE}
# gather information about sample corpus
docs.sample.info.df <- data.frame(FileName = character(), 
                                  NumberOfRows = integer(),
                                  NumberOfWords = integer(),
                                  NumberOfTermsPerDocument = integer())
for(i in 1:docs.num) {
    docs.sample.info.df <- rbind(docs.sample.info.df, 
                                 data.frame(FileName = docs.sample[[i]]$meta$id, 
                                            NumberOfRows = length(docs.sample[[i]]$content),
                                            NumberOfWords = sum(dtm.sample.unigram$v[dtm.sample.unigram$i == i]),
                                            NumberOfTermsPerDocument = sum(dtm.sample.unigram$v != 0 & 
                                                                               dtm.sample.unigram$i == i)))
}

# show output of DocumentTermMatrix
dtm.sample.unigram
```

From the output we can see some useful information, including total number of terms.  
Here are the lengths of samples, number of words instances, number of terms (unique words) per each document:  

```{r, echo = FALSE}
# show calculated info
docs.sample.info.df
```

### 1. Some words are more frequent than others - what are the distributions of word frequencies?  

```{r Freq1, echo = FALSE}
# Create matrixes
dtms.sample.unigram.m <- as.matrix(dtms.sample.unigram)
dtms.sample.bigram.m <- as.matrix(dtms.sample.bigram)
dtms.sample.trigram.m <- as.matrix(dtms.sample.trigram)

# sort frequencies of terms in separate documents
unigram.df <- data.frame(Term = factor(), Freq = integer(), Doc = factor())
bigram.df <- data.frame(Term = factor(), Freq = integer(), Doc = factor())
trigram.df <- data.frame(Term = factor(), Freq = integer(), Doc = factor())
unigram.df.top <- unigram.df
bigram.df.top <- bigram.df
trigram.df.top <- trigram.df

for(i in 1:docs.num) {
    # Unigrams
    doc <- data.frame(Term = dimnames(dtms.sample.unigram.m)$Terms, 
                      Freq = dtms.sample.unigram.m[i, ], 
                      Doc = as.factor(i))
    unigram.df <- rbind(unigram.df, 
                        doc[order(doc$Freq, decreasing = TRUE), ])

    unigram.df.top <- rbind(unigram.df.top, 
                            unigram.df[unigram.df$Doc == i, ][1:20, ])

    # Bigrams
    doc <- data.frame(Term = dimnames(dtms.sample.bigram.m)$Terms, 
                      Freq = dtms.sample.bigram.m[i, ], 
                      Doc = as.factor(i))
    bigram.df <- rbind(bigram.df, 
                        doc[order(doc$Freq, decreasing = TRUE), ])

    bigram.df.top <- rbind(bigram.df.top, 
                           bigram.df[bigram.df$Doc == i, ][1:20, ])
    # Trigrams
    doc <- data.frame(Term = dimnames(dtms.sample.trigram.m)$Terms, 
                      Freq = dtms.sample.trigram.m[i, ], 
                      Doc = as.factor(i))
    trigram.df <- rbind(trigram.df, 
                        doc[order(doc$Freq, decreasing = TRUE), ])

    trigram.df.top <- rbind(trigram.df.top, 
                            trigram.df[trigram.df$Doc == i, ][1:20, ])
}

# separate by docs
unigram.df.top.blogs <- unigram.df.top[unigram.df.top$Doc == 1, ]
unigram.df.top.blogs$Term <- factor(unigram.df.top.blogs$Term, 
                                    levels = unique(as.character(unigram.df.top.blogs$Term)))

unigram.df.top.news <- unigram.df.top[unigram.df.top$Doc == 2, ]
unigram.df.top.news$Term <- factor(unigram.df.top.news$Term, 
                                    levels = unique(as.character(unigram.df.top.news$Term)))

unigram.df.top.twitter <- unigram.df.top[unigram.df.top$Doc == 3, ]
unigram.df.top.twitter$Term <- factor(unigram.df.top.twitter$Term, 
                                    levels = unique(as.character(unigram.df.top.twitter$Term)))

bigram.df.top.blogs <- bigram.df.top[bigram.df.top$Doc == 1, ]
bigram.df.top.blogs$Term <- factor(bigram.df.top.blogs$Term, 
                                    levels = unique(as.character(bigram.df.top.blogs$Term)))
bigram.df.top.news <- bigram.df.top[bigram.df.top$Doc == 2, ]
bigram.df.top.news$Term <- factor(bigram.df.top.news$Term, 
                                    levels = unique(as.character(bigram.df.top.news$Term)))
bigram.df.top.twitter <- bigram.df.top[bigram.df.top$Doc == 3, ]
bigram.df.top.twitter$Term <- factor(bigram.df.top.twitter$Term, 
                                    levels = unique(as.character(bigram.df.top.twitter$Term)))

trigram.df.top.blogs <- trigram.df.top[trigram.df.top$Doc == 1, ]
trigram.df.top.blogs$Term <- factor(trigram.df.top.blogs$Term, 
                                    levels = unique(as.character(trigram.df.top.blogs$Term)))
trigram.df.top.news <- trigram.df.top[trigram.df.top$Doc == 2, ]
trigram.df.top.news$Term <- factor(trigram.df.top.news$Term, 
                                    levels = unique(as.character(trigram.df.top.news$Term)))
trigram.df.top.twitter <- trigram.df.top[trigram.df.top$Doc == 3, ]
trigram.df.top.twitter$Term <- factor(trigram.df.top.twitter$Term, 
                                    levels = unique(as.character(trigram.df.top.twitter$Term)))

# calculate aggregated frequency
unigram.freq <- colSums(dtms.sample.unigram.m)
bigram.freq <- colSums(dtms.sample.bigram.m)
trigram.freq <- colSums(dtms.sample.trigram.m)
# sort by most frequently used words
unigram.freq <- sort(unigram.freq, decreasing = TRUE)
bigram.freq <- sort(bigram.freq, decreasing = TRUE)
trigram.freq <- sort(trigram.freq, decreasing = TRUE)

unigram.terms <- names(unigram.freq)
bigram.terms <- names(bigram.freq)
trigram.terms <- names(trigram.freq)
# dataframe for more convenient use in ggplot
unigram.freq.df <- data.frame(Term = factor(unigram.terms, 
                                            levels = unigram.terms), 
                              Freq = unigram.freq)

bigram.freq.df <- data.frame(Term = factor(bigram.terms, 
                                            levels = bigram.terms), 
                              Freq = bigram.freq)

trigram.freq.df <- data.frame(Term = factor(trigram.terms, 
                                            levels = trigram.terms), 
                              Freq = trigram.freq)
```

Here is the histogram to understand the distribuition of word frequencies. Due to the distribution properties, it does not look good without transformations and in order to get a better looking histogram, the distribution is shown on the log scale.  
It is important to mention that sparse terms have been removed at earlier stage of documents processing. Black line shows **log(median)** which is around **`r round(log(median(unigram.freq.df$Freq)), 2)`** which corresponds to **median** = **`r round(median(unigram.freq.df$Freq), 2)`**.  
This means that after removing sparse terms, half of all the words in sample documents occur less than or equal to `r round(median(unigram.freq.df$Freq), 2)` times.

```{r displayHist1, echo = FALSE}

# terms frequencies histogram
qplot(log(Freq), data = unigram.freq.df, geom = "histogram", bins = 30, fill=..count..) +
    labs(x = "log(Term Frequency)",
         y = "Count",
         title = "Histogram of term frequencies") +
    geom_vline(xintercept = log(median(unigram.freq.df$Freq)), size = 2, color = "black")
```

Below are the barplots showing unigram terms frequencies per each document (Blogs, News and Twitter) separately and aggregated for all three documents (lower-right graph).  

```{r displayFreq1, fig.width = 9, fig.height = 7, echo = FALSE}
# graphs with term frequencies per document
g.perdoc1.bar <- ggplot(data = unigram.df.top.blogs, 
                       aes(Term, Freq, fill = Freq)) + 
    geom_bar(stat = "identity") + 
    labs(x = "Terms",
         y = "Frequency",
         title = "Most frequent terms in Blogs") +
    theme(axis.text.x=element_text(angle = 75, hjust = 1))

g.perdoc2.bar <- ggplot(data = unigram.df.top.news, 
                       aes(Term, Freq, fill = Freq)) + 
    geom_bar(stat = "identity") + 
    labs(x = "Terms",
         y = "Frequency",
         title = "Most frequent terms in News") +
    theme(axis.text.x=element_text(angle = 75, hjust = 1))

g.perdoc3.bar <- ggplot(data = unigram.df.top.twitter, 
                       aes(Term, Freq, fill = Freq)) + 
    geom_bar(stat = "identity") + 
    labs(x = "Terms",
         y = "Frequency",
         title = "Most frequent terms in Twitter") +
    theme(axis.text.x=element_text(angle = 75, hjust = 1))

# graph with most frequent terms aggregated for all document
g.agg.bar <- ggplot(data = unigram.freq.df[1:20, ], 
                    aes(Term, Freq, fill = Freq)) +
    geom_bar(stat = "identity") +
    labs(x = "Terms",
         y = "Frequency",
         title = "Most frequent terms in all documents") +
    theme(axis.text.x=element_text(angle = 75, hjust = 1))

graphs <- arrangeGrob(g.perdoc1.bar, g.perdoc2.bar, g.perdoc3.bar, g.agg.bar, ncol = 2, nrow = 2)
grid.draw(graphs)
```

For quick perception of the terms frequencies one can have a look at words cloud:  

```{r displayWC1, echo = FALSE}
# draw wordcloud
wordcloud(words = unigram.terms[1:100], 
          freq = unigram.freq[1:100], 
          random.order = FALSE, 
          scale = c(8, 1),
          min.freq = 1,
          rot.per = 0.35, 
          use.r.layout = FALSE,
          colors = brewer.pal(8, "Dark2"))
```

### 2. What are the frequencies of 2-grams and 3-grams in the dataset?  
Similar to unigram terms, here are the barplots for bigrams:  

```{r displayFreq2, fig.width = 9, fig.height = 7, echo = FALSE}
## Bigrams
# graphs with term frequencies per document
g.perdoc1.bar <- ggplot(data = bigram.df.top.blogs, 
                       aes(Term, Freq, fill = Freq)) + 
    geom_bar(stat = "identity") + 
    labs(x = "Terms",
         y = "Frequency",
         title = "Most frequent terms in Blogs") +
    theme(axis.text.x=element_text(angle = 75, hjust = 1))

g.perdoc2.bar <- ggplot(data = bigram.df.top.news, 
                       aes(Term, Freq, fill = Freq)) + 
    geom_bar(stat = "identity") + 
    labs(x = "Terms",
         y = "Frequency",
         title = "Most frequent terms in News") +
    theme(axis.text.x=element_text(angle = 75, hjust = 1))

g.perdoc3.bar <- ggplot(data = bigram.df.top.twitter, 
                       aes(Term, Freq, fill = Freq)) + 
    geom_bar(stat = "identity") + 
    labs(x = "Terms",
         y = "Frequency",
         title = "Most frequent terms in Twitter") +
    theme(axis.text.x=element_text(angle = 75, hjust = 1))

# graph with most frequent terms aggregated for all document
g.agg.bar <- ggplot(data = bigram.freq.df[1:20, ], 
                    aes(Term, Freq, fill = Freq)) +
    geom_bar(stat = "identity") +
    labs(x = "Terms",
         y = "Frequency",
         title = "Most frequent terms in all documents") +
    theme(axis.text.x=element_text(angle = 75, hjust = 1))

graphs <- arrangeGrob(g.perdoc1.bar, g.perdoc2.bar, g.perdoc3.bar, g.agg.bar, ncol = 2, nrow = 2)
grid.draw(graphs)
```

```{r displayWC2, echo = FALSE, eval = FALSE}
# draw wordcloud
wordcloud(words = bigram.terms[1:100], 
          freq = bigram.freq[1:100], 
          random.order = FALSE, 
          scale = c(4, 0.3),
          min.freq = 1,
          rot.per = 0.35, 
          use.r.layout = FALSE,
          colors = brewer.pal(8, "Dark2"))
```

Same way of presentation for trigram term frequencies. The difference of most frequent terms between different documents (types of sources) is significant especially copared to unigram frequencies above.  

```{r displayFreq3, fig.width = 9, fig.height = 7, echo = FALSE}
## Trigrams
# graphs with term frequencies per document
g.perdoc1.bar <- ggplot(data = trigram.df.top.blogs, 
                       aes(Term, Freq, fill = Freq)) + 
    geom_bar(stat = "identity") + 
    labs(x = "Terms",
         y = "Frequency",
         title = "Most frequent terms in Blogs") +
    theme(axis.text.x=element_text(angle = 75, hjust = 1))

g.perdoc2.bar <- ggplot(data = trigram.df.top.news, 
                       aes(Term, Freq, fill = Freq)) + 
    geom_bar(stat = "identity") + 
    labs(x = "Terms",
         y = "Frequency",
         title = "Most frequent terms in News") +
    theme(axis.text.x=element_text(angle = 75, hjust = 1))

g.perdoc3.bar <- ggplot(data = trigram.df.top.twitter, 
                       aes(Term, Freq, fill = Freq)) + 
    geom_bar(stat = "identity") + 
    labs(x = "Terms",
         y = "Frequency",
         title = "Most frequent terms in Twitter") +
    theme(axis.text.x=element_text(angle = 75, hjust = 1))

# graph with most frequent terms aggregated for all document
g.agg.bar <- ggplot(data = trigram.freq.df[1:20, ], 
                    aes(Term, Freq, fill = Freq)) +
    geom_bar(stat = "identity") +
    labs(x = "Terms",
         y = "Frequency",
         title = "Most frequent terms in all documents") +
    theme(axis.text.x=element_text(angle = 75, hjust = 1))

graphs <- arrangeGrob(g.perdoc1.bar, g.perdoc2.bar, g.perdoc3.bar, g.agg.bar, ncol = 2, nrow = 2)
grid.draw(graphs)
```

```{r displayWC3, echo = FALSE, eval = FALSE}
# draw wordcloud
wordcloud(words = trigram.terms[1:100], 
          freq = trigram.freq[1:100], 
          random.order = FALSE, 
          scale = c(3, 0.2),
          min.freq = 1,
          rot.per = 0.35, 
          use.r.layout = FALSE,
          colors = brewer.pal(8, "Dark2"))
```

### 3. How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?  
```{r, echo = FALSE}
# using dtm with sparse terms
dtm.sample.unigram.m <- as.matrix(dtm.sample.unigram)

# calculate frequency
frequency <- colSums(dtm.sample.unigram.m)

# sort by most frequently used words
frequency <- sort(frequency, decreasing = TRUE)

words.total <- sum(frequency)
words.agg <- 0
words.count <- 1

while(words.agg < 0.5 * words.total) {
    words.agg <- words.agg + frequency[words.count]
    words.count <- words.count + 1
}

words.count50 <- words.count

while(words.agg < 0.9 * words.total) {
    words.agg <- words.agg + frequency[words.count]
    words.count <- words.count + 1
}

words.count90 <- words.count
```

Number of frequent words covering half of the language: **`r as.integer(words.count50)`**  
Number of frequent words covering 90% of the language: **`r as.integer(words.count90)`**  

### 4. How do you evaluate how many of the words come from foreign languages?  

One of the ways to identify foreign words is by looking up for a word in a language dictionary. This approach is quite straighforward and has disadvantages such as incorrect classification of misspelled words.  
Another possible way is using machine learning algorithms with language profiles. This approach is used in *langid* library.  

### 5. Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?  

One of the ways of identifying words that may not be in the corpora is by using an algorithm which uses external dictionary with linguistic markers such as type of the word (noun, verb, adjective, adverb, etc.) together with some machine learning algorithms that can suggest the word.  
Using a smaller number of words in the dictionary to cover the same number of phrases can be possible by stemming words and suggesting endings based on some algorithm which takes into account grammar rules of the language.  

## References
1. [Text Mining Infrastructure in R](https://www.jstatsoft.org/article/view/v025i05)
2. [Basic Text Mining in R](https://rstudio-pubs-static.s3.amazonaws.com/31867_8236987cf0a8444e962ccd2aec46d9c3.html)
3. [Natural Language Processing](https://www.coursera.org/course/nlp)
